# /user_profiler/reflexio/server
Description: FastAPI backend server that processes user interactions to generate profiles, extract feedback, and evaluate agent success

## Main Entry Points

- **API**: `api.py` - FastAPI routes (only place to expose endpoints)
- **Endpoint Helpers**: `api_endpoints/` - Bridge between routes and business logic
- **Core Service**: `services/generation_service.py` - Main orchestrator

## Cache

**Directory**: `cache/`

| File | Purpose |
|------|---------|
| `reflexio_cache.py` | TTL-cached Reflexio instances (1 hour TTL, max 100 orgs) |

**Key Functions**:
- `get_reflexio(org_id)` - Get or create cached instance
- `invalidate_reflexio_cache(org_id)` - Invalidate after config changes
- `clear_reflexio_cache()` - Clear entire cache (testing/admin)

**Pattern**: **ALWAYS use `get_reflexio()`** instead of `Reflexio()` directly in API endpoints

## API Endpoints

**Directory**: `api_endpoints/`

| File | Purpose |
|------|---------|
| `request_context.py` | RequestContext (bundles org_id, storage, configurator, prompt_manager) |
| `publisher_api.py` | Publishing user interactions |
| `retriever_api.py` | Retrieving profiles, interactions, requests |
| `login.py` | Authentication with TTL-cached org lookups (5 min TTL), email verification, password reset |
| `precondition_checks.py` | Request validation |

**Key Endpoints**:
- `POST /api/publish_interaction` - Publish interactions (triggers profile/feedback/evaluation)
- `POST /api/get_requests` - Get request groups with associated interactions
- `GET /api/get_all_interactions` - Get all interactions across all users
- `GET /api/get_profile_statistics` - Profile statistics by status
- `GET /api/get_all_profiles?status_filter=<status>` - Filter by status (current/pending/archived)
- `POST /api/rerun_profile_generation` - Regenerate profiles from ALL interactions (creates PENDING, runs in background)
- `POST /api/manual_profile_generation` - Regenerate profiles from window-sized interactions (creates CURRENT)
- `POST /api/upgrade_all_profiles` - PENDING → CURRENT, delete old ARCHIVED
- `POST /api/downgrade_all_profiles` - ARCHIVED → CURRENT, demote PENDING
- `POST /api/add_raw_feedback` - Add raw feedback directly to storage
- `POST /api/rerun_feedback_generation` - Regenerate feedback for agent version (creates PENDING, runs in background)
- `POST /api/manual_feedback_generation` - Regenerate feedback from window-sized interactions (creates CURRENT)
- `POST /api/run_feedback_aggregation` - Aggregate raw feedbacks into insights
- `POST /api/run_skill_generation` - Generate skills from clustered raw feedbacks (expensive, 5/min) **[gated by `skill_generation` feature flag]**
- `POST /api/get_skills` - List skills (filtered by feedback_name, agent_version, skill_status) **[gated]**
- `POST /api/search_skills` - Hybrid search skills (vector + FTS) **[gated]**
- `POST /api/update_skill_status` - Update skill status (DRAFT → PUBLISHED → DEPRECATED) **[gated]**
- `DELETE /api/delete_skill` - Delete a skill by ID **[gated]**
- `POST /api/export_skills` - Export skills as SKILL.md markdown **[gated]**
- `POST /api/search` - Unified search across profiles, feedbacks, raw_feedbacks, skills (parallel, with query rewriting) **[query rewrite gated by `query_rewrite` flag]**
- `POST /api/upgrade_all_raw_feedbacks` - PENDING → CURRENT for raw feedbacks
- `POST /api/downgrade_all_raw_feedbacks` - ARCHIVED → CURRENT for raw feedbacks
- `DELETE /api/delete_feedback` - Delete feedback by ID
- `DELETE /api/delete_raw_feedback` - Delete raw feedback by ID
- `GET /api/get_operation_status` - Get background operation status
- `POST /api/cancel_operation` - Cancel an in-progress operation (rerun or manual generation)

**Login Response**: `POST /token` now returns `feature_flags: dict[str, bool]` alongside `api_key` and `token_type`. Frontend stores these in localStorage to gate UI features.

**Authentication Endpoints**:
- `POST /api/register` - Register new org (accepts optional `invitation_code` form field; when `invitation_only` flag enabled, code is required and org is auto-verified)
- `POST /api/verify-email` - Verify email with token
- `POST /api/resend-verification` - Resend verification email
- `POST /api/forgot-password` - Request password reset email
- `POST /api/reset-password` - Reset password with token

**Self-Host Mode** (`SELF_HOST=true`): No auth, default org: `self-host-org`, requires S3 config storage (`CONFIG_S3_*` env vars)

**Pattern**: All endpoints call `Reflexio` from `reflexio_lib.py`

## Database

**Directory**: `db/`

Key files:
- `database.py`: Connection setup (priority: S3 in self-host → Supabase → SQLite)
- `db_models.py`: SQLAlchemy models (users, tokens, configs, invitation codes)
- `db_operations.py`: CRUD operations with retry logic (tenacity: 3 attempts, exponential backoff), invitation code management (claim/release/create)
- `login_supabase_client.py`: Cloud Supabase client for auth (see `supabase_login/README.md`)
- `s3_org_storage.py`: S3-based organization storage for self-host mode (singleton, cached in memory)

**Connection Priority** (in `database.py` and `db_operations.py`):
1. **S3** - Self-host mode with `CONFIG_S3_*` env vars → Uses `S3OrganizationStorage`
2. **Supabase** - `LOGIN_SUPABASE_URL` + `LOGIN_SUPABASE_KEY` → Uses Supabase client
3. **SQLite** - Fallback for local development

**Self-Host Mode** (`SELF_HOST=true`): Requires S3 configuration
- All org/auth data stored in S3 via `S3OrganizationStorage`
- S3 file: `auth/organizations.json` (with optional Fernet encryption)
- No local database needed

**Note**: Only for auth/config, NOT for profiles/interactions (those use Storage)

## LLM Client

**Directory**: `llm/`
**Entry Point**: `litellm_client.py` - `LiteLLMClient`

Key files:
- `litellm_client.py`: Unified LiteLLMClient using LiteLLM for multi-provider support
- `openai_client.py`: OpenAI implementation (legacy, do not use directly)
- `claude_client.py`: Claude implementation (legacy, do not use directly)
- `llm_utils.py`: Helper functions for Pydantic model conversion

**Features**:
- Uses LiteLLM for multi-provider support (OpenAI, Claude, Azure, OpenRouter, Gemini, etc.)
- **Gemini support**: Model names with `gemini/` prefix route through Google Gemini; API key from `api_key_config.gemini`
- **OpenRouter support**: Model names with `openrouter/` prefix (e.g., `openrouter/openai/gpt-5-nano`) route through OpenRouter; API key from `api_key_config.openrouter`
- API keys read from environment variables (OPENAI_API_KEY, ANTHROPIC_API_KEY) or `ApiKeyConfig`
- Interface: `generate_response()`, `generate_chat_response()`, `get_embedding()`
- **Structured Outputs**: Supports Pydantic models via `response_format` parameter
- Return types: `str` for text, or `BaseModel` for Pydantic models

**Usage**:
```python
from reflexio.server.llm.litellm_client import LiteLLMClient, LiteLLMConfig

# Create client
config = LiteLLMConfig(model="gpt-4o-mini")
client = LiteLLMClient(config)

# Text response
response = client.generate_response("Hello")  # Returns str

# Structured output with Pydantic model
from pydantic import BaseModel
class Answer(BaseModel):
    result: int
response = client.generate_response("What is 2+2?", response_format=Answer)  # Returns Answer instance
```

**Rules**:
- **ALWAYS use `LiteLLMClient`**, never import `OpenAIClient` or `ClaudeClient` directly
- **ALWAYS use Pydantic models** for structured outputs (dict-based schemas are not supported)

## Prompts

**Directory**: `prompt/`

Key components:
- `prompt_manager.py`: PromptManager for loading and rendering
- `prompt_bank/`: Templates by prompt_id (metadata.json + version.prompt files)

**Pattern**: Access via `request_context.prompt_manager.render_prompt(prompt_id, variables)`

## Site Variables

**Directory**: `site_var/`

See `site_var/README.md` for detailed documentation.

| File | Purpose |
|------|---------|
| `site_var_manager.py` | SiteVarManager (singleton) - loads JSON/TXT configs |
| `feature_flags.py` | Per-org feature gating (`is_feature_enabled()`, `get_all_feature_flags()`) |

**Feature Flags**: Config in `site_var_sources/feature_flags.json`. Each flag has global `enabled` toggle and per-org `enabled_org_ids` allowlist. Unknown flags default to enabled (fail-open). Currently gates: `skill_generation` (all skill endpoints return 403 when disabled), `invitation_only` (global flag, gates registration to require invitation codes), `query_rewrite` (LLM-based query expansion for search, disabled by default).

Access: `SiteVarManager().get_site_var(key)` for raw values, `feature_flags.is_feature_enabled(org_id, name)` for flag checks

## Email Service

**Directory**: `services/email/`

| File | Purpose |
|------|---------|
| `email_service.py` | EmailService - sends emails via AWS SES |
| `templates/` | Email templates (verification, password reset) |

**Features**:
- Email verification for new registrations
- Password reset emails
- Uses AWS SES (requires `AWS_REGION`, `SES_SENDER_EMAIL` env vars)

**Usage**: `get_email_service()` from `api.py` (lazy-loaded singleton)

## Scripts

**Directory**: `scripts/`

| File | Purpose |
|------|---------|
| `manage_invitation_codes.py` | CLI to generate and list invitation codes |

**Usage**:
```shell
python -m reflexio.server.scripts.manage_invitation_codes generate --count 5
python -m reflexio.server.scripts.manage_invitation_codes generate --count 3 --expires-in-days 30
python -m reflexio.server.scripts.manage_invitation_codes list
python -m reflexio.server.scripts.manage_invitation_codes list --show-used
```

## Services

**Directory**: `services/`

### Orchestrator

**File**: `generation_service.py` - GenerationService

Main orchestrator flow:
1. Save interactions to storage
2. Run ProfileGenerationService, FeedbackGenerationService, AgentSuccessEvaluationService in parallel (ThreadPoolExecutor, 3 workers)

**Timeout Protection**: Two-layer timeout strategy:
- **Service level**: `GENERATION_SERVICE_TIMEOUT_SECONDS = 600` (10 min) — outer timeout for each parallel service
- **Extractor level**: `EXTRACTOR_TIMEOUT_SECONDS = 300` (5 min) — per-extractor safety net in `base_generation_service.py`
- If one service/extractor times out, others continue unaffected

**Stride Processing**: Each extractor independently checks if it should run based on its configured stride size and tracks its own operation state.

Called by API endpoints via `Reflexio`

### Base Infrastructure

- `base_generation_service.py`: Abstract base for all services (parallel extractor execution via ThreadPoolExecutor, `EXTRACTOR_TIMEOUT_SECONDS = 300` per-extractor safety timeout)
- `extractor_config_utils.py`: Shared utility for filtering extractor configs by source, `allow_manual_trigger`, and extractor names
- `extractor_interaction_utils.py`: Per-extractor utilities for stride checking and source filtering
- `operation_state_utils.py`: Centralized `OperationStateManager` for all `_operation_state` table interactions (progress tracking, concurrency locks, extractor/aggregator bookmarks, simple locks)
- `deduplication_utils.py`: Base deduplicator class for LLM-based semantic matching (used by ProfileDeduplicator and FeedbackDeduplicator)
- `service_utils.py`: Utilities (`construct_messages_from_interactions()`, `format_interactions_to_history_string()` (prepends tool usage info when `tools_used` is present), `extract_json_from_string()`, `log_model_response()` for colored LLM response logging)

**Operation State Management** (via `OperationStateManager` in `operation_state_utils.py`):
- Centralized manager for all `_operation_state` table interactions with 6 use cases:
  1. **Progress tracking**: Rerun + manual batch operations (key: `{service}::{org_id}::progress`)
  2. **Concurrency lock**: Atomic lock with request queuing (key: `{service}::{org_id}[::scope_id]::lock`)
  3. **Extractor bookmark**: Track last-processed interactions per extractor (key: `{service}::{org_id}[::scope_id]::{name}`)
  4. **Aggregator bookmark**: Track last-processed raw_feedback_id per aggregator
  4b. **Cluster fingerprints**: Track cluster membership fingerprints for change detection (key: `{service}::{org_id}::{name}[::version]::clusters`)
  5. **Simple lock**: Non-queuing lock for cleanup operations
  6. **Cancellation**: Cooperative cancellation for batch operations (`request_cancellation()`, `is_cancellation_requested()`, `mark_cancelled()`). Uses separate DB row (key: `{service}::{org_id}::cancellation`) to avoid lost-update race conditions with progress updates.
- Stale lock timeout: 5 minutes (assumes crashed if lock held longer)
- Lock scoping: Profile generation = per-user, Feedback generation = per-org
- Re-run mechanism: If new request arrives during generation, `pending_request_id` is set and generation re-runs after completion

### Profile Generation

**Directory**: `services/profile/`

Key files:
- `profile_generation_service.py`: Service orchestrator
- `profile_extractor.py`: Extractor that generates profile updates
- `profile_updater.py`: Applies updates (add/delete/mention) to storage
- `profile_deduplicator.py`: Merges duplicate profiles from multiple extractors using LLM

**Flow**: Interactions → ProfileExtractor → ProfileDeduplicator (optional) → ProfileUpdater → Storage

**Generation Modes** (detailed comparison):

| Aspect | Regular | Rerun | Manual Regular |
|--------|---------|-------|----------------|
| **Trigger** | Auto (on publish) | Manual (API) | Manual (API) |
| **Stride Check** | Yes (skips if below threshold) | No (always runs) | No (always runs) |
| **Interactions** | Window-sized (last k) | Window-sized (last k) | Window-sized (last k) |
| **Time Range Filter** | No | Yes (optional start/end) | No |
| **Pre-processing** | None | None | None |
| **Existing Profile Context** | All profiles loaded | Only PENDING profiles loaded | All profiles loaded |
| **Output Status** | CURRENT | PENDING | CURRENT |
| **Scope** | Single user | Batch (all matching users, with progress) | Batch (all/single user, with progress) |
| **Use Case** | Normal operation | Test prompt changes | Force regeneration |

**Note**: All modes use `extraction_window_size` (per-extractor override or global). The key difference is that Regular checks stride before running, while Rerun/Manual always run. When no window is configured, rerun/manual falls back to `k=1000`.

**Constructor Flags** (`ProfileGenerationService`):
- `allow_manual_trigger`: Include `manual_trigger=True` extractors (default: False)
- `output_pending_status`: Set output profiles to PENDING status (default: False)

**Profile Versioning Workflow**:

Users can regenerate and manage profile versions using a four-state system:

1. **CURRENT** (status=None): Active profiles shown to users
2. **PENDING** (status="pending"): Newly generated profiles awaiting review
3. **ARCHIVED** (status="archived"): Previous version of profiles
4. **ARCHIVE_IN_PROGRESS** (status="archive_in_progress"): Temporary status during downgrade operation

**Rerun Workflow**:
```
1. Rerun Generation → Creates PENDING profiles (existing CURRENT unchanged)
2. Review PENDING → Compare new vs current profiles
3. Upgrade → CURRENT→ARCHIVED, PENDING→CURRENT, delete old ARCHIVED
4. OR Downgrade → CURRENT→ARCHIVED (restore previous version), ARCHIVED→CURRENT (swap)
```

**Upgrade Process** (3 atomic steps):
1. Archive all CURRENT profiles → ARCHIVED
2. Promote all PENDING profiles → CURRENT
3. Delete all old ARCHIVED profiles

**Downgrade Process** (3 atomic steps):
1. Mark all CURRENT profiles → ARCHIVE_IN_PROGRESS (temporary)
2. Restore all ARCHIVED profiles → CURRENT
3. Complete archiving: ARCHIVE_IN_PROGRESS → ARCHIVED

**Use Cases**:
- Test prompt changes without affecting production profiles
- Review AI-generated updates before deployment
- Rollback to previous profile version if needed

### Feedback Extraction

**Directory**: `services/feedback/`

See `services/feedback/README.md` for detailed component documentation.

Key files:
- `feedback_generation_service.py`: Service orchestrator
- `feedback_extractor.py`: Extractor that extracts raw feedback
- `feedback_aggregator.py`: Aggregates similar raw feedbacks (with cluster-level change detection to skip unchanged clusters)
- `feedback_deduplicator.py`: Merges duplicate feedbacks from multiple extractors using LLM
- `skill_generator.py`: Generates rich skills from clustered raw feedbacks enriched with interaction context

**Flow**:
- Interactions → FeedbackExtractor → FeedbackDeduplicator (optional) → RawFeedback (with optional `blocking_issue`) → Storage
- RawFeedback (manual trigger) → FeedbackAggregator → cluster fingerprint comparison → LLM only for changed clusters → Feedback (with optional `blocking_issue`) → Storage
- RawFeedback → SkillGenerator (clusters + interaction enrichment + LLM) → Skill → Storage

**Tool Analysis**: FeedbackExtractor reads `tool_can_use` from root `Config` and passes it to prompts for tool usage analysis and blocking issue detection.

**Rerun Behavior**: Groups interactions by `user_id` for per-user feedback extraction (fetches all users, then processes each user's interactions together)

**Feedback Aggregation with Cluster Change Detection** (`feedback_aggregator.py`):

Aggregation clusters raw feedbacks by embedding similarity, then calls LLM per cluster to produce aggregated feedback. Cluster-level change detection avoids redundant LLM calls on subsequent runs:

1. Cluster all raw feedbacks (agglomerative for <50, HDBSCAN for >=50)
2. Compute fingerprint per cluster (SHA-256 of sorted `raw_feedback_id`s, 16 hex chars)
3. Compare against stored fingerprints from previous run (via `OperationStateManager.get_cluster_fingerprints`)
4. Only call LLM for changed/new clusters; carry forward existing feedbacks for unchanged clusters
5. Archive old feedbacks only for changed/disappeared clusters (via `archive_feedbacks_by_ids`)
6. Store new fingerprints with feedback_id mapping (via `OperationStateManager.update_cluster_fingerprints`)

| Scenario | Behavior |
|---|---|
| First run (no stored fingerprints) | All clusters treated as changed, full LLM run |
| `rerun=True` | Bypasses fingerprint comparison, full archive/regenerate |
| No changes | Logs skip message, updates bookmark, returns early |
| Error during save | Restores only selectively archived feedbacks |

**Generation Modes** (detailed comparison):

| Aspect | Regular | Rerun | Manual Regular |
|--------|---------|-------|----------------|
| **Trigger** | Auto (on publish) | Manual (API) | Manual (API) |
| **Stride Check** | Yes (skips if below threshold) | No (always runs) | No (always runs) |
| **Interactions** | Window-sized (last k) | Window-sized (last k) | Window-sized (last k) |
| **Time Range Filter** | No | Yes (optional start/end) | No |
| **Pre-processing** | None | Deletes existing PENDING raw feedbacks | None |
| **Output Status** | CURRENT | PENDING | CURRENT |
| **Scope** | Single user | Batch (all matching users, with progress) | Batch (all/single user, with progress) |
| **Use Case** | Normal operation | Test prompt changes | Force regeneration |

**Note**: All modes use `extraction_window_size` (per-extractor override or global). The key difference is that Regular checks stride before running, while Rerun/Manual always run. When no window is configured, rerun/manual falls back to `k=1000`.

**Constructor Flags** (`FeedbackGenerationService`):
- `allow_manual_trigger`: Include `manual_trigger=True` extractors (default: False)
- `output_pending_status`: Set output raw feedbacks to PENDING status (default: False)

**Raw Feedback Versioning Workflow**:

Similar to profiles, raw feedbacks support versioning:

1. **CURRENT** (status=None): Active raw feedbacks
2. **PENDING** (status="pending"): Newly generated raw feedbacks awaiting review
3. **ARCHIVED** (status="archived"): Previous version of raw feedbacks

**Rerun Workflow**:
```
1. Rerun Feedback Generation → Creates PENDING raw feedbacks
2. Review PENDING → Compare new vs current
3. Upgrade → CURRENT→ARCHIVED, PENDING→CURRENT, delete old ARCHIVED
4. OR Downgrade → Swap ARCHIVED↔CURRENT
```

### Agent Success Evaluation

**Directory**: `services/agent_success_evaluation/`

Key files:
- `agent_success_evaluation_service.py`: Service orchestrator
- `agent_success_evaluator.py`: Evaluates success and improvement areas
- `agent_success_evaluation_constants.py`: Output schemas (`AgentSuccessEvaluationOutput`, `AgentSuccessEvaluationWithComparisonOutput`)
- `agent_success_evaluation_utils.py`: Message construction utilities

**Flow**: Interactions → AgentSuccessEvaluator → AgentSuccessEvaluationResult → Storage

**Tool Context**: Reads `tool_can_use` from root `Config` level (shared with feedback extraction).

**Shadow Comparison Mode**: When interactions contain `shadow_content`, evaluator automatically:
1. Randomly assigns regular/shadow to Request 1/2 (avoids position bias)
2. Evaluates regular version for success
3. Compares regular vs shadow to determine which is better
4. Returns `regular_vs_shadow` field with values: `REGULAR_IS_BETTER`, `REGULAR_IS_SLIGHTLY_BETTER`, `SHADOW_IS_BETTER`, `SHADOW_IS_SLIGHTLY_BETTER`, `TIED`

### Query Rewriter

**File**: `services/query_rewriter.py` - `QueryRewriter`

Expands user search queries with synonyms via LLM for improved full-text search recall. Uses `websearch_to_tsquery` syntax (supports OR, phrases, negation). Gated behind the `query_rewrite` feature flag.

- Uses `query_rewrite_model_name` from `llm_model_setting.json` (fast, cheap model)
- Structured output via `RewrittenQuery` Pydantic model
- Falls back to original query on any failure
- Prompt: `prompt_bank/query_rewrite/`

### Unified Search Service

**File**: `services/unified_search_service.py` - `run_unified_search()`

Searches across all entity types (profiles, feedbacks, raw_feedbacks, skills) in parallel via a two-phase approach:

- **Phase A**: Query rewriting + embedding generation (parallel via ThreadPoolExecutor)
- **Phase B**: Entity searches across all types (parallel via ThreadPoolExecutor, 4 workers)

Skills search gated behind `skill_generation` feature flag. Pre-computed embeddings passed to storage methods via `query_embedding` parameter to avoid redundant embedding calls.

### Storage

**Directory**: `services/storage/`

| File | Purpose |
|------|---------|
| `storage_base.py` | BaseStorage abstract class |
| `supabase_storage.py` | Production storage with vector embeddings (parses `blocking_issue` JSONB for feedbacks) |
| `supabase_storage_utils.py` | Helpers: data conversion (handles `tools_used`/`blocking_issue` JSONB serialization), SQL migration runner |
| `supabase_migrations.py` | Data migrations that run alongside SQL schema migrations |
| `local_json_storage.py` | Local file-based for testing |

**Pattern**: **NEVER import SupabaseStorage/LocalJsonStorage directly** - Always use `request_context.storage`

**Key Methods**:
- CRUD: profiles, interactions, feedbacks, results, requests, skills
- `get_request_groups()` → `dict[str, list[RequestInteractionDataModel]]` (groups by request_id)
- `get_feedbacks(status_filter, feedback_status_filter)` - Filter by profile status and approval status
- `save_feedbacks()` → returns `list[Feedback]` with `feedback_id` populated (callers can ignore return)
- Selective feedback operations (used by cluster change detection):
  - `archive_feedbacks_by_ids(feedback_ids)` - Archive specific feedbacks by ID (skips APPROVED)
  - `restore_archived_feedbacks_by_ids(feedback_ids)` - Restore archived feedbacks by ID
  - `delete_feedbacks_by_ids(feedback_ids)` - Delete feedbacks by ID
- Vector search via LiteLLMClient embeddings
- Operation state: `get_operation_state()`, `upsert_operation_state()`, `get_operation_state_with_new_request_interaction()`, `try_acquire_in_progress_lock()`
- All operation state interactions are managed through `OperationStateManager` (in `operation_state_utils.py`)
- Profile status: `Status` enum (CURRENT=None, PENDING, ARCHIVED)

### Configurator

**Directory**: `services/configurator/`

Key files:
- `configurator.py`: SimpleConfigurator - loads YAML config, creates storage
- `local_json_config_storage.py`: Local file-based config storage
- `rds_config_storage.py`: RDS database config storage (default for cloud)
- `s3_config_storage.py`: S3-based config storage with optional encryption

**Config Storage Priority** (in `SimpleConfigurator`):
1. **Local** - If `base_dir` is explicitly provided (testing)
2. **S3** - If all `CONFIG_S3_*` env vars are set (required in self-host mode)
3. **RDS** - Default fallback (not available in self-host mode)

**Path Handling**: LocalJsonConfigStorage automatically converts relative paths to absolute using `os.path.abspath()`

Access: `request_context.configurator`

## Architecture Patterns

### Request Flow
```
API Request (api.py)
  -> API Endpoint (api_endpoints/)
    -> get_reflexio() (cache/)
      -> Reflexio (reflexio_lib.py)
        -> GenerationService
          ├─> ProfileGenerationService → Storage
          ├─> FeedbackGenerationService → Storage
          └─> AgentSuccessEvaluationService → Storage
```

```mermaid
flowchart TB
    subgraph API["API Layer"]
        A[api.py] --> B[api_endpoints/]
    end

    B --> C[get_reflexio]
    C --> D[Reflexio]
    D --> E[GenerationService]

    subgraph ProfileService["ProfileGenerationService"]
        E --> F1[ProfileExtractor 1]
        E --> F2[ProfileExtractor N]
        F1 --> PD[ProfileDeduplicator]
        F2 --> PD
        PD --> PU[ProfileUpdater]
    end

    subgraph FeedbackService["FeedbackGenerationService"]
        E --> G1[FeedbackExtractor 1]
        E --> G2[FeedbackExtractor N]
        G1 --> FD[FeedbackDeduplicator]
        G2 --> FD
        FD -.->|auto trigger| SG[SkillGenerator]
    end

    subgraph EvalService["AgentSuccessEvaluationService"]
        E --> H1[AgentSuccessEvaluator 1]
        E --> H2[AgentSuccessEvaluator N]
    end

    PU --> I[(Storage)]
    FD --> I
    H1 --> I
    H2 --> I

    subgraph Support["Supporting Components"]
        J[LiteLLMClient]
        K[PromptManager]
        L[Configurator]
    end

    J -.-> F1
    J -.-> G1
    J -.-> H1
    J -.-> PD
    J -.-> FD
    K -.-> F1
    K -.-> G1
    K -.-> H1
```

### Service Pattern

All services follow BaseGenerationService:
1. Load extractor configs from YAML
2. Load generation service config from request (runtime parameters)
3. Filter extractors by source, `allow_manual_trigger`, and extractor names (via `extractor_config_utils`)
4. Create extractors with both configs
5. Run extractors in parallel (ThreadPoolExecutor)
6. Process and save results to storage

**Extractor Pattern**: Multiple extractors run in parallel, each handling its own data collection. Each extractor:
- Receives **ExtractorConfig** (from YAML): Static configuration like prompts and settings
- Receives **GenerationServiceConfig** (from request): Runtime parameters like user_id, source
- **Collects its own interactions** using `extractor_interaction_utils.py`:
  - Gets per-extractor window/stride parameters (override or global fallback)
  - Applies source filtering based on `request_sources_enabled`
  - Checks stride threshold before running
  - Updates per-extractor bookmark state after processing (via `OperationStateManager`)

**Per-Extractor Window Overrides**: Each extractor config can override global window settings:
- `extraction_window_size_override`: Override global `extraction_window_size` for this extractor
- `extraction_window_stride_override`: Override global `extraction_window_stride` for this extractor
- Each extractor applies its own override or falls back to global values

### Key Rules

**Reflexio Instances**:
- **NEVER instantiate `Reflexio()` directly** in API endpoints
- **ALWAYS use**: `get_reflexio(org_id)` from `cache/reflexio_cache.py`
- Cache invalidated automatically on config changes

**Storage**:
- **NEVER import SupabaseStorage/LocalJsonStorage directly**
- **ALWAYS use**: `request_context.storage` (type: BaseStorage)

**LLM**:
- **NEVER import OpenAIClient/ClaudeClient directly**
- **ALWAYS use**: `LiteLLMClient` (uses LiteLLM for multi-provider support)

**Prompts**:
- **NEVER hardcode prompts**
- **ALWAYS use**: `request_context.prompt_manager.render_prompt(prompt_id, variables)`
- Prompts versioned in `prompt_bank/`
