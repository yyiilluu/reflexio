"""
Base class for generation services
"""

import enum
import logging
import os
import time
import uuid
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from typing import Any, Generic, Optional, TypeVar

from reflexio_commons.api_schema.internal_schema import RequestInteractionDataModel
from reflexio_commons.api_schema.service_schemas import Status

from reflexio.server.api_endpoints.request_context import RequestContext
from reflexio.server.llm.litellm_client import LiteLLMClient
from reflexio.server.services.extractor_config_utils import filter_extractor_configs
from reflexio.server.services.extractor_interaction_utils import (
    get_effective_source_filter,
    get_extractor_window_params,
)
from reflexio.server.services.operation_state_utils import OperationStateManager
from reflexio.server.services.service_utils import log_model_response


class StatusChangeOperation(str, enum.Enum):
    """Operation type for upgrade/downgrade responses."""

    UPGRADE = "upgrade"
    DOWNGRADE = "downgrade"


class ExtractorExecutionError(RuntimeError):
    """Raised when all extractors fail for a request/user context."""


logger = logging.getLogger(__name__)

# Timeout for individual extractor execution (safety net if LLM provider ignores its own timeout)
EXTRACTOR_TIMEOUT_SECONDS = 300

# Type variables for generic base service
TExtractorConfig = TypeVar(
    "TExtractorConfig"
)  # Extractor config type from YAML (e.g., AgentFeedbackConfig, ProfileExtractorConfig)
TExtractor = TypeVar(
    "TExtractor"
)  # Extractor type (e.g., FeedbackExtractor, AgentSuccessEvaluator, ProfileExtractor)
TResult = TypeVar("TResult")  # Result type (e.g., RawFeedback, ProfileUpdates)
TGenerationServiceConfig = TypeVar(
    "TGenerationServiceConfig"
)  # Runtime service configuration type (e.g., FeedbackGenerationServiceConfig, ProfileGenerationServiceConfig)
TRequest = TypeVar(
    "TRequest"
)  # Request type (e.g., ProfileGenerationRequest, FeedbackGenerationRequest, AgentSuccessEvaluationRequest)


# Unified base class for all generation services (evaluation, feedback, profile)
class BaseGenerationService(
    ABC, Generic[TExtractorConfig, TExtractor, TGenerationServiceConfig, TRequest]
):
    """
    Base class for generation services that run multiple extractors sequentially.

    This unified class supports two types of services:
    1. Evaluation services (feedback, agent success) - process interactions and save RawFeedback
    2. Profile services - process interactions with existing data and apply updates

    Type Parameters:
        TExtractorConfig: The extractor configuration type from YAML (e.g., AgentFeedbackConfig, ProfileExtractorConfig)
        TExtractor: The extractor type (e.g., FeedbackExtractor, ProfileExtractor, AgentSuccessEvaluator)
        TGenerationServiceConfig: The runtime service configuration type (e.g., FeedbackGenerationServiceConfig, ProfileGenerationServiceConfig)
        TRequest: The request type (e.g., ProfileGenerationRequest, FeedbackGenerationRequest, AgentSuccessEvaluationRequest)

    Child classes must implement:
    - _load_extractor_configs(): Load extractor configurations from configurator
    - _load_generation_service_config(): Extract parameters from request and return GenerationServiceConfig
    - _create_extractor(): Create extractor instances with extractor config and service config
    - _get_service_name(): Get service name for logging
    - _process_results(): Process and save results (can access self.service_config)
    """

    def __init__(
        self, llm_client: LiteLLMClient, request_context: RequestContext
    ) -> None:
        """
        Initialize the base generation service.

        Args:
            llm_client: Unified LLM client supporting both OpenAI and Claude
            request_context: Request context with storage, configurator, and org_id
        """
        self.client = llm_client
        self.storage = request_context.storage
        self.org_id = request_context.org_id
        self.configurator = request_context.configurator
        self.request_context = request_context
        self.service_config: Optional[TGenerationServiceConfig] = None
        self._is_batch_mode: bool = False
        self._last_extractor_run_stats: dict[str, int] = {
            "total": 0,
            "failed": 0,
            "timed_out": 0,
        }

    @abstractmethod
    def _load_extractor_configs(self) -> list[TExtractorConfig]:
        """
        Load extractor configurations from the configurator.

        Returns:
            List of extractor configuration objects (from YAML)
        """

    @abstractmethod
    def _load_generation_service_config(
        self, request: TRequest
    ) -> TGenerationServiceConfig:
        """
        Extract parameters from request object and return GenerationServiceConfig.

        Args:
            request: The request object

        Returns:
            GenerationServiceConfig object (e.g., FeedbackGenerationServiceConfig, ProfileGenerationServiceConfig)
        """

    @abstractmethod
    def _create_extractor(
        self,
        extractor_config: TExtractorConfig,
        service_config: TGenerationServiceConfig,
    ) -> TExtractor:
        """
        Create an extractor instance from extractor config and service config.

        Args:
            extractor_config: The extractor configuration object from YAML (e.g., AgentFeedbackConfig, ProfileExtractorConfig)
            service_config: The runtime service configuration object (e.g., FeedbackGenerationServiceConfig, ProfileGenerationServiceConfig)

        Returns:
            An extractor instance
        """

    @abstractmethod
    def _get_service_name(self) -> str:
        """
        Get the name of the service for logging purposes.

        Returns:
            Service name string
        """

    @abstractmethod
    def _get_base_service_name(self) -> str:
        """
        Get the base service name for OperationStateManager keys.

        This is the service identity used for progress/lock key construction,
        independent of whether the operation is a rerun or regular run.

        Returns:
            Base service name (e.g., "profile_generation", "feedback_generation")
        """

    @abstractmethod
    def _process_results(self, results: list) -> None:
        """
        Process and save all results from extractors. Called once after all extractors complete.

        Responsible for flattening, deduplication (if applicable), and saving results.
        Can access self.service_config for context.

        Args:
            results: List of all results from extractors (one per successful extractor)
        """

    @abstractmethod
    def _should_track_in_progress(self) -> bool:
        """
        Return True if this service should track in-progress state to prevent duplicates.

        Profile and Feedback services should return True to prevent duplicate generation
        when back-to-back requests arrive. AgentSuccess services should return False
        as they process per-request and don't have the same duplication issue.

        Returns:
            bool: True if in-progress tracking should be enabled
        """

    @abstractmethod
    def _get_lock_scope_id(self, request: TRequest) -> Optional[str]:
        """
        Get the scope ID for lock key construction.

        Profile services return user_id (per-user lock), feedback services return None (per-org lock).

        Args:
            request: The generation request

        Returns:
            Optional[str]: Scope ID (e.g., user_id) or None for org-level scope
        """

    def _filter_extractor_configs_by_service_config(
        self,
        extractor_configs: list[TExtractorConfig],
        service_config: TGenerationServiceConfig,
    ) -> list[TExtractorConfig]:
        """
        Filter extractor configs based on request_sources_enabled and manual_trigger fields.

        Args:
            extractor_configs: List of extractor configuration objects from YAML
            service_config: Runtime service configuration containing the source and allow_manual_trigger flag

        Returns:
            Filtered list of extractor configs that should run for the given source and trigger mode
        """
        # Extract filtering parameters from service_config
        source = getattr(service_config, "source", None)
        allow_manual_trigger = getattr(service_config, "allow_manual_trigger", False)
        extractor_names = getattr(service_config, "extractor_names", None)

        return filter_extractor_configs(
            extractor_configs=extractor_configs,
            source=source,
            allow_manual_trigger=allow_manual_trigger,
            extractor_names=extractor_names,
        )

    # ===============================
    # In-progress state management via OperationStateManager
    # ===============================

    def _create_state_manager(self) -> OperationStateManager:
        """Create an OperationStateManager for this service.

        Returns:
            OperationStateManager instance configured for this service
        """
        return OperationStateManager(
            self.storage, self.org_id, self._get_base_service_name()
        )

    def run(self, request: TRequest) -> None:
        """
        Run the generation service for the given request.

        This is the main entry point that:
        1. If in-progress tracking is enabled, handles lock acquisition/release
        2. Validates and extracts parameters from the request into GenerationServiceConfig
        3. Runs extractors sequentially (each extractor handles its own data collection)
        4. Processes results
        5. Re-runs if new requests came in during generation

        Args:
            request: The request object containing parameters
        """
        # Check if this service tracks in-progress state
        if not self._should_track_in_progress():
            self._run_generation(request)
            return

        # Get scope ID and request ID for in-progress tracking
        scope_id = self._get_lock_scope_id(request)
        my_request_id = getattr(request, "request_id", None) or str(uuid.uuid4())

        state_manager = self._create_state_manager()

        # Try to acquire lock
        if not state_manager.acquire_lock(my_request_id, scope_id=scope_id):
            return  # Another operation is running, we've updated pending_request_id

        # Re-run loop: keep running until no new requests come in
        try:
            while True:
                self._run_generation(request)

                # If in batch mode and cancellation was requested, clear lock
                # to prevent queued pending requests from running, then stop
                if self._is_batch_mode and state_manager.is_cancellation_requested():
                    state_manager.clear_lock(scope_id=scope_id)
                    logger.info(
                        "Cancellation detected in run() for %s, cleared lock to prevent pending re-runs",
                        self._get_service_name(),
                    )
                    break

                # Check if another request came in during our run
                pending_request_id = state_manager.release_lock(
                    my_request_id, scope_id=scope_id
                )

                logger.info(
                    "Released in-progress lock for %s: request_id=%s, pending_request_id=%s",
                    self._get_service_name(),
                    my_request_id,
                    pending_request_id,
                )

                if not pending_request_id:
                    break  # No pending request, we're done

                # Another request came in, update my_request_id and re-run
                my_request_id = pending_request_id

        except Exception:
            # Clear lock on error to prevent deadlock
            state_manager.clear_lock(scope_id=scope_id)
            raise

    def _run_generation(self, request: TRequest) -> None:
        """
        Run the actual generation logic.

        This method contains the core generation logic extracted from the original run() method.
        It handles:
        1. Validating and extracting parameters from the request
        2. Running extractors sequentially
        3. Processing results

        Args:
            request: The request object containing parameters
        """
        # Validate request
        if not request:
            logger.error("Received None request for %s", self._get_service_name())
            return

        try:
            # Extract parameters into GenerationServiceConfig
            self.service_config = self._load_generation_service_config(request)

            # Load extractor configs
            extractor_configs = self._load_extractor_configs()
            if not extractor_configs:
                logger.warning(
                    "No %s extractor configs found", self._get_service_name()
                )
                return

            # Filter configs based on source and manual trigger (if applicable)
            extractor_configs = self._filter_extractor_configs_by_service_config(
                extractor_configs, self.service_config
            )

            if not extractor_configs:
                source = getattr(self.service_config, "source", "N/A")
                source_display = source if source else "N/A"
                logger.info(
                    "No %s extractor configs enabled for source: %s",
                    self._get_service_name(),
                    source_display,
                )
                return

            # Get identifier for error context
            identifier = getattr(self.service_config, "user_id", None) or getattr(
                self.service_config, "request_id", "unknown"
            )

            # Pre-extraction check (e.g., consolidated should_generate)
            if not self._should_run_before_extraction(extractor_configs):
                logger.info(
                    "Pre-extraction check returned False for %s identifier=%s, skipping",
                    self._get_service_name(),
                    identifier,
                )
                return

            # Run extractors sequentially: each extractor runs independently,
            # then existing_data is refreshed so the next extractor sees updated state.
            # Results are collected and processed once after all extractors complete.
            all_results = []
            previously_extracted = []
            run_stats = {"total": len(extractor_configs), "failed": 0, "timed_out": 0}

            for i, config in enumerate(extractor_configs):
                if i > 0 and previously_extracted:
                    # Re-fetch existing_data from storage (picks up saved results from previous extractor)
                    self.service_config = self._load_generation_service_config(request)
                    # Let subclass update config for incremental mode
                    self._update_config_for_incremental(previously_extracted)

                extractor = self._create_extractor(config, self.service_config)
                executor: Optional[ThreadPoolExecutor] = None
                try:
                    executor = ThreadPoolExecutor(max_workers=1)
                    future = executor.submit(extractor.run)
                    result = future.result(timeout=EXTRACTOR_TIMEOUT_SECONDS)
                    if result:
                        all_results.append(result)
                        previously_extracted.append(result)
                except FuturesTimeoutError:
                    run_stats["failed"] += 1
                    run_stats["timed_out"] += 1
                    logger.error(
                        "Extractor timed out after %d seconds for %s identifier=%s",
                        EXTRACTOR_TIMEOUT_SECONDS,
                        self._get_service_name(),
                        identifier,
                    )
                    continue
                except Exception as e:
                    run_stats["failed"] += 1
                    logger.error(
                        "Extractor failed for %s identifier=%s: %s (type=%s)",
                        self._get_service_name(),
                        identifier,
                        str(e),
                        type(e).__name__,
                    )
                    continue
                finally:
                    if executor is not None:
                        executor.shutdown(wait=False, cancel_futures=True)

            self._last_extractor_run_stats = run_stats

            # Check if all extractors failed
            if not all_results:
                all_extractors_failed = (
                    run_stats["total"] > 0 and run_stats["failed"] == run_stats["total"]
                )
                if all_extractors_failed:
                    error_msg = (
                        f"All extractors failed for {self._get_service_name()} "
                        f"identifier={identifier}"
                    )
                    logger.error(error_msg)
                    raise ExtractorExecutionError(error_msg)
                logger.info(
                    "No results generated for %s identifier: %s",
                    self._get_service_name(),
                    identifier,
                )
                return

            # Process all results once after all extractors complete
            self._process_results(all_results)

        except Exception as e:
            logger.error(
                "Failed to run %s due to %s, exception type: %s",
                self._get_service_name(),
                str(e),
                type(e).__name__,
            )
            if isinstance(e, ExtractorExecutionError):
                raise

    def _should_run_before_extraction(
        self, extractor_configs: list[TExtractorConfig]
    ) -> bool:
        """
        Pre-extraction check called before the sequential extraction loop.

        Template method that:
        1. Skips for non-auto runs and mock mode
        2. Collects scoped interactions via _collect_scoped_interactions_for_precheck
        3. Delegates prompt building to _build_should_run_prompt (subclass hook)
        4. Makes a single LLM call to determine if extraction should proceed

        Override _build_should_run_prompt in subclasses to provide service-specific
        criteria and prompt construction. Default returns True (always run) when
        no prompt hook is provided.

        Args:
            extractor_configs: List of enabled extractor configs that will be run

        Returns:
            bool: True if extraction should proceed, False to skip
        """
        # Skip for non-auto runs (rerun/manual flows always run)
        if not getattr(self.service_config, "auto_run", True):
            return True

        # Skip for mock mode
        if os.getenv("MOCK_LLM_RESPONSE", "").lower() == "true":
            return True

        # Collect scoped interactions
        request_groups, scoped_configs = self._collect_scoped_interactions_for_precheck(
            extractor_configs
        )
        if not request_groups:
            logger.info(
                "No interactions found for consolidated should_generate check for %s",
                self._get_service_name(),
            )
            return False

        # Build prompt via subclass hook
        prompt = self._build_should_run_prompt(scoped_configs, request_groups)
        if not prompt:
            return True  # No prompt means no check needed, proceed

        # Resolve model and make LLM call
        should_run_model = self._resolve_should_run_model()
        identifier = getattr(self.service_config, "user_id", None) or "unknown"
        try:
            should_start = time.perf_counter()
            logger.info(
                "event=consolidated_should_run_start service=%s identifier=%s model=%s extractors=%d",
                self._get_service_name(),
                identifier,
                should_run_model,
                len(extractor_configs),
            )
            logger.info("Should extract prompt: %s", prompt)

            content = self.client.generate_chat_response(
                messages=[{"role": "user", "content": prompt}],
                model=should_run_model,
            )
            log_model_response(
                logger,
                f"Consolidated {self._get_service_name()} should_run response",
                content,
            )
            decision = bool(content and "true" in content.lower())
            logger.info(
                "event=consolidated_should_run_end service=%s identifier=%s elapsed_seconds=%.3f decision=%s",
                self._get_service_name(),
                identifier,
                time.perf_counter() - should_start,
                decision,
            )
            return decision
        except Exception as exc:
            logger.error(
                "Consolidated should_generate check failed for %s: %s, defaulting to run",
                self._get_service_name(),
                str(exc),
            )
            return True

    def _build_should_run_prompt(
        self,
        scoped_configs: list[TExtractorConfig],
        request_groups: list[RequestInteractionDataModel],
    ) -> Optional[str]:
        """
        Build the prompt for the consolidated should_run LLM check.

        Override in subclasses to provide service-specific criteria building
        and prompt rendering. Return None if no check is needed (always proceed).

        Args:
            scoped_configs: Extractor configs that had scoped interactions
            request_groups: Deduplicated request interaction groups

        Returns:
            Optional[str]: The rendered prompt string, or None to skip the check
        """
        return None

    def _collect_scoped_interactions_for_precheck(
        self, extractor_configs: list[TExtractorConfig]
    ) -> tuple[list[RequestInteractionDataModel], list[TExtractorConfig]]:
        """
        Collect interactions for consolidated pre-check using extractor-scoped filters.

        Mirrors each extractor's source/window scope so the consolidated gate
        does not skip valid extraction because of an unrelated fixed interaction slice.

        Args:
            extractor_configs: Enabled extractor configs after request-level filtering

        Returns:
            tuple: (deduplicated request groups, extractor configs that had scoped interactions)
        """
        root_config = self.request_context.configurator.get_config()
        global_window_size = (
            getattr(root_config, "extraction_window_size", None)
            if root_config
            else None
        )
        global_stride = (
            getattr(root_config, "extraction_window_stride", None)
            if root_config
            else None
        )

        deduped_request_groups: dict[str, RequestInteractionDataModel] = {}
        scoped_configs: list[TExtractorConfig] = []
        extra_kwargs = self._get_precheck_interaction_query_kwargs()

        for config in extractor_configs:
            should_skip, effective_source = get_effective_source_filter(
                config, getattr(self.service_config, "source", None)
            )
            if should_skip:
                continue

            window_size, _ = get_extractor_window_params(
                config, global_window_size, global_stride
            )
            fetch_k = window_size if window_size and window_size > 0 else 1000
            request_groups, _ = self.storage.get_last_k_interactions_grouped(
                user_id=getattr(self.service_config, "user_id", None),
                k=fetch_k,
                sources=effective_source,
                start_time=getattr(self.service_config, "rerun_start_time", None),
                end_time=getattr(self.service_config, "rerun_end_time", None),
                **extra_kwargs,
            )
            if not request_groups:
                continue

            scoped_configs.append(config)
            for request_group in request_groups:
                request_id = getattr(request_group.request, "request_id", None)
                dedupe_key = (
                    request_id
                    or request_group.request_group
                    or f"scoped_group_{len(deduped_request_groups)}"
                )
                if dedupe_key not in deduped_request_groups:
                    deduped_request_groups[dedupe_key] = request_group

        return list(deduped_request_groups.values()), scoped_configs

    def _get_precheck_interaction_query_kwargs(self) -> dict:
        """
        Return extra keyword arguments for get_last_k_interactions_grouped in precheck.

        Override in subclasses that need additional query parameters
        (e.g., agent_version for feedback services).

        Returns:
            dict: Extra kwargs to pass to get_last_k_interactions_grouped
        """
        return {}

    def _resolve_should_run_model(self) -> str:
        """
        Resolve the model name for should_run/should_generate LLM checks.

        Uses LLM config override if available, falls back to site var setting.

        Returns:
            str: Model name for the should_run check
        """
        root_config = self.request_context.configurator.get_config()
        llm_config = root_config.llm_config if root_config else None
        from reflexio.server.site_var.site_var_manager import SiteVarManager

        model_setting = SiteVarManager().get_site_var("llm_model_setting")
        return (
            llm_config.should_run_model_name
            if llm_config and llm_config.should_run_model_name
            else model_setting.get("should_run_model_name", "gpt-5-nano")
        )

    def _update_config_for_incremental(self, previously_extracted: list) -> None:
        """
        Update service_config for incremental extraction after the first extractor.

        Override in subclasses that support incremental extraction to set
        is_incremental and previously_extracted on the service config.
        Default implementation does nothing.

        Args:
            previously_extracted: List of results from previous extractors
        """

    # ===============================
    # Batch with progress (shared by rerun + manual)
    # ===============================

    def _run_batch_with_progress(
        self,
        user_ids: list[str],
        request: TRequest,
        request_params: dict,
        state_manager: OperationStateManager,
    ) -> tuple[int, int]:
        """Run a batch of users with progress tracking.

        Shared logic for both run_rerun() and run_manual_regular().
        Initializes progress, processes each user, and finalizes.
        Checks for cancellation before each user.

        Args:
            user_ids: List of user IDs to process
            request: The original request object
            request_params: Parameters dict for progress state
            state_manager: OperationStateManager instance

        Returns:
            Tuple of (users_processed, total_generated)
        """
        total_users = len(user_ids)
        self._is_batch_mode = True

        # Initialize progress
        state_manager.initialize_progress(
            total_users=total_users,
            request_params=request_params,
        )

        try:
            # Process each user
            users_processed = 0
            for user_id in user_ids:
                # Check for cancellation before starting next user
                if state_manager.is_cancellation_requested():
                    logger.info(
                        "Cancellation requested for %s, stopping after %d/%d users",
                        self._get_base_service_name(),
                        users_processed,
                        total_users,
                    )
                    state_manager.mark_cancelled()
                    return users_processed, self._get_generated_count(request)

                state_manager.set_current_item(user_id)

                try:
                    run_request = self._create_run_request_for_item(user_id, request)
                    self.run(run_request)
                    users_processed += 1

                    state_manager.update_progress(
                        item_id=user_id,
                        count=0,  # Extractors collect their own data
                        success=True,
                        total_users=total_users,
                    )

                except Exception as e:
                    logger.error(
                        "Failed to process user %s for %s: %s",
                        user_id,
                        self._get_base_service_name(),
                        str(e),
                    )
                    state_manager.update_progress(
                        item_id=user_id,
                        count=0,
                        success=False,
                        total_users=total_users,
                        error=str(e),
                    )
                    continue

            # Get generated count and finalize
            total_generated = self._get_generated_count(request)
            state_manager.finalize_progress(users_processed, total_generated)

            return users_processed, total_generated
        finally:
            self._is_batch_mode = False

    # ===============================
    # Rerun methods (optional - override to enable rerun functionality)
    # ===============================

    def _get_rerun_user_ids(self, request: TRequest) -> list[str]:
        """Get user IDs to process during rerun.

        Override this method to enable rerun functionality for the service.
        Returns a list of user IDs that have interactions matching the request filters.
        Each extractor collects its own data using its configured window_size.

        Args:
            request: The rerun request object

        Returns:
            List of user IDs to process
        """
        raise NotImplementedError("Rerun not supported by this service")

    def _build_rerun_request_params(self, request: TRequest) -> dict:
        """Build request params dict for operation state tracking.

        Override this method to enable rerun functionality for the service.

        Args:
            request: The rerun request object

        Returns:
            Dictionary of request parameters for state tracking
        """
        raise NotImplementedError("Rerun not supported by this service")

    def _create_run_request_for_item(self, user_id: str, request: TRequest) -> TRequest:
        """Create the request object to pass to self.run() for a single user.

        Override this method to enable rerun functionality for the service.
        Each extractor collects its own data using its configured window_size.

        Args:
            user_id: The user ID to process
            request: The original rerun request object

        Returns:
            A request object suitable for self.run()
        """
        raise NotImplementedError("Rerun not supported by this service")

    def _create_rerun_response(self, success: bool, msg: str, count: int) -> Any:
        """Create the rerun response object.

        Override this method to enable rerun functionality for the service.

        Args:
            success: Whether the operation succeeded
            msg: Status message
            count: Number of items generated

        Returns:
            A response object (e.g., RerunProfileGenerationResponse)
        """
        raise NotImplementedError("Rerun not supported by this service")

    def _get_generated_count(self, request: TRequest) -> int:
        """Get the count of generated items (profiles or feedbacks) after rerun.

        Override this method to enable rerun functionality for the service.

        Args:
            request: The rerun request object (for filtering)

        Returns:
            Number of items generated during rerun
        """
        raise NotImplementedError("Rerun not supported by this service")

    def _pre_process_rerun(self, request: TRequest) -> None:
        """Hook called before processing rerun items.

        Override in subclasses to perform cleanup or preparation before rerun.
        Default implementation does nothing.

        Args:
            request: The rerun request object
        """

    def run_rerun(self, request: TRequest) -> Any:
        """Run the rerun workflow for the service.

        This template method orchestrates the rerun process:
        1. Check for existing in-progress operations
        2. Get user IDs to process
        3. Pre-process hook
        4. Run batch with progress tracking
        5. Return response

        Child classes must implement the hook methods to enable rerun functionality:
        - _get_rerun_user_ids()
        - _build_rerun_request_params()
        - _create_run_request_for_item()
        - _create_rerun_response()

        Args:
            request: The rerun request object

        Returns:
            A response object with success status, message, and count
        """
        state_manager = self._create_state_manager()

        try:
            # 1. Check for existing in-progress operation
            error = state_manager.check_in_progress()
            if error:
                return self._create_rerun_response(False, error, 0)

            # 2. Get user IDs to process
            user_ids = self._get_rerun_user_ids(request)
            if not user_ids:
                return self._create_rerun_response(
                    False, "No interactions found matching the specified filters", 0
                )

            # 3. Pre-process hook (e.g., delete existing pending items)
            self._pre_process_rerun(request)

            # 4. Run batch with progress tracking
            users_processed, total_generated = self._run_batch_with_progress(
                user_ids=user_ids,
                request=request,
                request_params=self._build_rerun_request_params(request),
                state_manager=state_manager,
            )

            msg = f"Completed for {users_processed} user(s)"
            return self._create_rerun_response(True, msg, total_generated)

        except Exception as e:
            state_manager.mark_progress_failed(str(e))
            return self._create_rerun_response(
                False,
                f"Failed to run {self._get_base_service_name()}: {str(e)}",
                0,
            )

    # ===============================
    # Upgrade/Downgrade methods (optional - override to enable)
    # ===============================

    def _has_items_with_status(
        self, status: Optional[Status], request: TRequest
    ) -> bool:
        """Check if items exist with given status and filters from request.

        Override this method to enable upgrade/downgrade functionality for the service.

        Args:
            status: The status to check for (None for CURRENT)
            request: The upgrade/downgrade request object with filters

        Returns:
            bool: True if any matching items exist
        """
        raise NotImplementedError("Upgrade/downgrade not supported by this service")

    def _delete_items_by_status(self, status: Status, request: TRequest) -> int:
        """Delete items with given status matching request filters.

        Override this method to enable upgrade/downgrade functionality for the service.

        Args:
            status: The status of items to delete
            request: The upgrade/downgrade request object with filters

        Returns:
            int: Number of items deleted
        """
        raise NotImplementedError("Upgrade/downgrade not supported by this service")

    def _update_items_status(
        self,
        old_status: Optional[Status],
        new_status: Optional[Status],
        request: TRequest,
        user_ids: Optional[list[str]] = None,
    ) -> int:
        """Update items from old_status to new_status with request filters.

        Override this method to enable upgrade/downgrade functionality for the service.

        Args:
            old_status: The current status to match (None for CURRENT)
            new_status: The new status to set (None for CURRENT)
            request: The upgrade/downgrade request object with filters
            user_ids: Optional pre-computed list of user IDs to filter by

        Returns:
            int: Number of items updated
        """
        raise NotImplementedError("Upgrade/downgrade not supported by this service")

    def _get_affected_user_ids_for_upgrade(
        self, request: TRequest
    ) -> Optional[list[str]]:
        """Get user IDs to filter by for upgrade operations.

        Override this method to support the only_affected_users flag.
        By default returns None (no filtering).

        Args:
            request: The upgrade request object

        Returns:
            Optional[list[str]]: List of user IDs to filter by, or None for no filtering
        """
        return None

    def _get_affected_user_ids_for_downgrade(
        self, request: TRequest
    ) -> Optional[list[str]]:
        """Get user IDs to filter by for downgrade operations.

        Override this method to support the only_affected_users flag.
        By default returns None (no filtering).

        Args:
            request: The downgrade request object

        Returns:
            Optional[list[str]]: List of user IDs to filter by, or None for no filtering
        """
        return None

    def _create_status_change_response(
        self,
        operation: StatusChangeOperation,
        success: bool,
        counts: dict,
        msg: str,
    ) -> Any:
        """Create upgrade or downgrade response object based on operation type.

        Override this method to enable upgrade/downgrade functionality for the service.

        Args:
            operation: The operation type (UPGRADE or DOWNGRADE)
            success: Whether the operation succeeded
            counts: Dictionary of counts (upgrade: deleted/archived/promoted, downgrade: demoted/restored)
            msg: Status message

        Returns:
            A response object (e.g., UpgradeProfilesResponse, DowngradeRawFeedbacksResponse)
        """
        raise NotImplementedError("Upgrade/downgrade not supported by this service")

    def run_upgrade(self, request: TRequest) -> Any:
        """Run the upgrade workflow for the service.

        This template method orchestrates the upgrade process:
        1. Validate that pending items exist
        2. Delete old archived items
        3. Archive current items (None → ARCHIVED)
        4. Promote pending items (PENDING → None/CURRENT)

        Child classes must implement the hook methods to enable upgrade functionality:
        - _has_items_with_status()
        - _delete_items_by_status()
        - _update_items_status()
        - _create_status_change_response()

        Args:
            request: The upgrade request object with optional filters

        Returns:
            A response object with success status, counts, and message
        """
        try:
            # 1. Validate pending items exist
            if not self._has_items_with_status(Status.PENDING, request):
                return self._create_status_change_response(
                    StatusChangeOperation.UPGRADE,
                    False,
                    {"deleted": 0, "archived": 0, "promoted": 0},
                    "No pending items found to upgrade",
                )

            # Get affected user IDs once (child class determines the logic)
            affected_user_ids = self._get_affected_user_ids_for_upgrade(request)

            # 2. Delete old archived items (skip if archive_current=False)
            deleted = 0
            archived = 0
            if getattr(request, "archive_current", True):
                deleted = self._delete_items_by_status(Status.ARCHIVED, request)

                # 3. Archive current items (None → ARCHIVED)
                archived = self._update_items_status(
                    None, Status.ARCHIVED, request, user_ids=affected_user_ids
                )

            # 4. Promote pending items (PENDING → None)
            promoted = self._update_items_status(
                Status.PENDING, None, request, user_ids=affected_user_ids
            )

            msg = f"Upgraded: {promoted} promoted, {archived} archived, {deleted} old archived deleted"
            return self._create_status_change_response(
                StatusChangeOperation.UPGRADE,
                True,
                {"deleted": deleted, "archived": archived, "promoted": promoted},
                msg,
            )

        except Exception as e:
            return self._create_status_change_response(
                StatusChangeOperation.UPGRADE,
                False,
                {"deleted": 0, "archived": 0, "promoted": 0},
                f"Failed to upgrade: {str(e)}",
            )

    def run_downgrade(self, request: TRequest) -> Any:
        """Run the downgrade workflow for the service.

        This template method orchestrates the downgrade process:
        1. Validate that archived items exist
        2. Demote current items (None → ARCHIVE_IN_PROGRESS)
        3. Restore archived items (ARCHIVED → None/CURRENT)
        4. Complete archiving (ARCHIVE_IN_PROGRESS → ARCHIVED)

        Child classes must implement the hook methods to enable downgrade functionality:
        - _has_items_with_status()
        - _update_items_status()
        - _create_status_change_response()

        Args:
            request: The downgrade request object with optional filters

        Returns:
            A response object with success status, counts, and message
        """
        try:
            # 1. Validate archived items exist
            if not self._has_items_with_status(Status.ARCHIVED, request):
                return self._create_status_change_response(
                    StatusChangeOperation.DOWNGRADE,
                    False,
                    {"demoted": 0, "restored": 0},
                    "No archived items found to restore",
                )

            # Get affected user IDs once (child class determines the logic)
            affected_user_ids = self._get_affected_user_ids_for_downgrade(request)

            # 2. Demote current (None → ARCHIVE_IN_PROGRESS)
            demoted = self._update_items_status(
                None, Status.ARCHIVE_IN_PROGRESS, request, user_ids=affected_user_ids
            )

            # 3. Restore archived (ARCHIVED → None)
            restored = self._update_items_status(
                Status.ARCHIVED, None, request, user_ids=affected_user_ids
            )

            # 4. Complete archiving (ARCHIVE_IN_PROGRESS → ARCHIVED)
            self._update_items_status(
                Status.ARCHIVE_IN_PROGRESS,
                Status.ARCHIVED,
                request,
                user_ids=affected_user_ids,
            )

            msg = f"Downgraded: {demoted} archived, {restored} restored"
            return self._create_status_change_response(
                StatusChangeOperation.DOWNGRADE,
                True,
                {"demoted": demoted, "restored": restored},
                msg,
            )

        except Exception as e:
            return self._create_status_change_response(
                StatusChangeOperation.DOWNGRADE,
                False,
                {"demoted": 0, "restored": 0},
                f"Failed to downgrade: {str(e)}",
            )
