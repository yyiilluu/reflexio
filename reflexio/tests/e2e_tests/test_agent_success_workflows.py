"""End-to-end tests for agent success evaluation workflows."""

from typing import Callable

from reflexio.reflexio_lib.reflexio_lib import Reflexio
from reflexio.tests.server.test_utils import skip_in_precommit, skip_low_priority
from reflexio_commons.api_schema.retriever_schema import (
    GetAgentSuccessEvaluationResultsRequest,
    GetDashboardStatsRequest,
)
from reflexio_commons.api_schema.service_schemas import (
    InteractionData,
    RegularVsShadow,
    UserActionType,
)


@skip_in_precommit
def test_publish_interaction_agent_success_only(
    reflexio_instance_agent_success_only: Reflexio,
    sample_interaction_requests: list[InteractionData],
    cleanup_after_test: Callable[[], None],
):
    """Test interaction publishing with only agent success evaluation enabled."""
    user_id = "test_user_agent_success_only"
    agent_version = "test_agent_success"

    # Publish interactions (request_id will be auto-generated)
    response = reflexio_instance_agent_success_only.publish_interaction(
        {
            "user_id": user_id,
            "interaction_data_list": sample_interaction_requests,
            "source": "test_conversation",
            "agent_version": agent_version,
        }
    )

    # Verify successful publication
    assert response.success is True
    assert response.message == ""

    # Verify interactions were added to storage
    final_interactions = (
        reflexio_instance_agent_success_only.request_context.storage.get_all_interactions()
    )
    assert len(final_interactions) == len(sample_interaction_requests)
    # Get the auto-generated request_id
    request_id = final_interactions[0].request_id

    # Verify agent success evaluation results were created
    agent_success_results = reflexio_instance_agent_success_only.request_context.storage.get_agent_success_evaluation_results(
        agent_version=agent_version
    )
    assert len(agent_success_results) > 0
    assert agent_success_results[0].request_id == request_id
    assert agent_success_results[0].agent_version == agent_version
    assert isinstance(agent_success_results[0].is_success, bool)

    # Verify NO profiles were generated (since profile config is not enabled)
    final_profiles = (
        reflexio_instance_agent_success_only.request_context.storage.get_all_profiles()
    )
    assert len(final_profiles) == 0

    # Verify NO profile change logs were created (since profile config is not enabled)
    final_change_logs = (
        reflexio_instance_agent_success_only.request_context.storage.get_profile_change_logs()
    )
    assert len(final_change_logs) == 0

    # Verify NO feedbacks were generated (since feedback config is not enabled)
    raw_feedbacks = reflexio_instance_agent_success_only.request_context.storage.get_raw_feedbacks(
        feedback_name="test_feedback"
    )
    assert len(raw_feedbacks) == 0


@skip_in_precommit
def test_get_agent_success_evaluations_end_to_end(
    reflexio_instance_agent_success_only: Reflexio,
    sample_interaction_requests: list[InteractionData],
    cleanup_agent_success_only: Callable[[], None],
):
    """Test end-to-end workflow for getting agent success evaluation results.

    This test verifies:
    1. Publishing interactions creates agent success evaluations
    2. get_agent_success_evaluation_results retrieves results correctly
    3. Filtering by agent_version works
    4. Limit parameter works correctly
    5. Results contain expected fields
    """
    user_id = "test_user_get_evaluations"
    agent_version = "test_agent_v1"

    # Step 1: Publish interactions to generate evaluations
    publish_response = reflexio_instance_agent_success_only.publish_interaction(
        {
            "user_id": user_id,
            "interaction_data_list": sample_interaction_requests,
            "source": "test_evaluations_source",
            "agent_version": agent_version,
        }
    )
    assert publish_response.success is True

    # Get the auto-generated request_id
    stored_interactions = (
        reflexio_instance_agent_success_only.request_context.storage.get_all_interactions()
    )
    assert len(stored_interactions) > 0
    request_id = stored_interactions[0].request_id

    # Step 2: Get agent success evaluations via API
    get_response = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            GetAgentSuccessEvaluationResultsRequest(agent_version=agent_version)
        )
    )
    assert get_response.success is True
    assert len(get_response.agent_success_evaluation_results) > 0

    # Step 3: Verify result fields
    result = get_response.agent_success_evaluation_results[0]
    assert result.request_id == request_id
    assert result.agent_version == agent_version
    assert isinstance(result.is_success, bool)
    assert isinstance(result.failure_type, str)
    assert isinstance(result.failure_reason, str)
    assert isinstance(result.agent_prompt_update, str)
    assert result.created_at > 0

    # Step 4: Test filtering by agent_version (non-existent version)
    empty_response = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            GetAgentSuccessEvaluationResultsRequest(
                agent_version="non_existent_version"
            )
        )
    )
    assert empty_response.success is True
    assert len(empty_response.agent_success_evaluation_results) == 0

    # Step 5: Test limit parameter
    limited_response = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            GetAgentSuccessEvaluationResultsRequest(
                agent_version=agent_version, limit=1
            )
        )
    )
    assert limited_response.success is True
    assert len(limited_response.agent_success_evaluation_results) <= 1

    # Step 6: Test with dict input
    dict_response = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            {"agent_version": agent_version, "limit": 10}
        )
    )
    assert dict_response.success is True
    assert len(dict_response.agent_success_evaluation_results) > 0


@skip_in_precommit
@skip_low_priority
def test_agent_success_evaluation_statistics(
    reflexio_instance_agent_success_only: Reflexio,
    sample_interaction_requests: list[InteractionData],
    cleanup_agent_success_only: Callable[[], None],
):
    """Test agent success evaluation statistics in dashboard.

    This test verifies:
    1. Dashboard stats include evaluation time series data
    2. Evaluation counts are reflected in statistics
    3. Time series data is properly formatted
    """
    user_id = "test_user_stats"
    agent_version = "test_agent_stats"

    # Step 1: Publish interactions to generate evaluations
    publish_response = reflexio_instance_agent_success_only.publish_interaction(
        {
            "user_id": user_id,
            "interaction_data_list": sample_interaction_requests,
            "source": "test_stats_source",
            "agent_version": agent_version,
        }
    )
    assert publish_response.success is True

    # Verify evaluations were created
    evaluations = reflexio_instance_agent_success_only.request_context.storage.get_agent_success_evaluation_results(
        agent_version=agent_version
    )
    assert len(evaluations) > 0

    # Step 2: Get dashboard statistics
    stats_response = reflexio_instance_agent_success_only.get_dashboard_stats(
        GetDashboardStatsRequest(days_back=7)
    )
    assert stats_response.success is True
    assert stats_response.stats is not None

    # Step 3: Verify evaluations time series exists
    assert stats_response.stats.evaluations_time_series is not None
    assert len(stats_response.stats.evaluations_time_series) >= 0

    # Step 4: Verify time series data format (if data exists)
    if len(stats_response.stats.evaluations_time_series) > 0:
        ts_point = stats_response.stats.evaluations_time_series[0]
        assert hasattr(ts_point, "timestamp")
        assert hasattr(ts_point, "value")

    # Step 5: Verify current period stats
    current_period = stats_response.stats.current_period
    assert current_period is not None
    # Interactions should be reflected
    assert current_period.total_interactions >= len(sample_interaction_requests)


@skip_in_precommit
@skip_low_priority
def test_multiple_agent_versions_evaluation(
    reflexio_instance_agent_success_only: Reflexio,
    sample_interaction_requests: list[InteractionData],
    cleanup_agent_success_only: Callable[[], None],
):
    """Test agent success evaluations across multiple agent versions.

    This test verifies:
    1. Different agent versions create separate evaluation records
    2. Filtering by agent_version returns only matching results
    3. Results without version filter return all versions
    4. Each version's evaluations are independent
    """
    user_id = "test_user_multi_version"
    agent_version_v1 = "test_agent_v1.0"
    agent_version_v2 = "test_agent_v2.0"
    agent_version_v3 = "test_agent_v3.0"

    # Step 1: Publish interactions with different agent versions
    # Version 1
    publish_v1 = reflexio_instance_agent_success_only.publish_interaction(
        {
            "user_id": user_id,
            "interaction_data_list": sample_interaction_requests[:2],
            "source": "source_v1",
            "agent_version": agent_version_v1,
        }
    )
    assert publish_v1.success is True

    # Version 2
    publish_v2 = reflexio_instance_agent_success_only.publish_interaction(
        {
            "user_id": f"{user_id}_v2",
            "interaction_data_list": sample_interaction_requests[1:3],
            "source": "source_v2",
            "agent_version": agent_version_v2,
        }
    )
    assert publish_v2.success is True

    # Version 3
    publish_v3 = reflexio_instance_agent_success_only.publish_interaction(
        {
            "user_id": f"{user_id}_v3",
            "interaction_data_list": sample_interaction_requests,
            "source": "source_v3",
            "agent_version": agent_version_v3,
        }
    )
    assert publish_v3.success is True

    # Step 2: Get evaluations for each version separately
    results_v1 = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            GetAgentSuccessEvaluationResultsRequest(agent_version=agent_version_v1)
        )
    )
    assert results_v1.success is True
    assert len(results_v1.agent_success_evaluation_results) > 0
    for result in results_v1.agent_success_evaluation_results:
        assert result.agent_version == agent_version_v1

    results_v2 = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            GetAgentSuccessEvaluationResultsRequest(agent_version=agent_version_v2)
        )
    )
    assert results_v2.success is True
    assert len(results_v2.agent_success_evaluation_results) > 0
    for result in results_v2.agent_success_evaluation_results:
        assert result.agent_version == agent_version_v2

    results_v3 = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            GetAgentSuccessEvaluationResultsRequest(agent_version=agent_version_v3)
        )
    )
    assert results_v3.success is True
    assert len(results_v3.agent_success_evaluation_results) > 0
    for result in results_v3.agent_success_evaluation_results:
        assert result.agent_version == agent_version_v3

    # Step 3: Get all evaluations without version filter
    all_results = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            GetAgentSuccessEvaluationResultsRequest(limit=100)
        )
    )
    assert all_results.success is True

    # Total should include all versions
    total_expected = (
        len(results_v1.agent_success_evaluation_results)
        + len(results_v2.agent_success_evaluation_results)
        + len(results_v3.agent_success_evaluation_results)
    )
    assert len(all_results.agent_success_evaluation_results) >= total_expected

    # Step 4: Verify version isolation - v1 results don't appear in v2 query
    v1_request_ids = {r.request_id for r in results_v1.agent_success_evaluation_results}
    v2_request_ids = {r.request_id for r in results_v2.agent_success_evaluation_results}
    v3_request_ids = {r.request_id for r in results_v3.agent_success_evaluation_results}

    # Request IDs should be unique across versions (different publishes)
    assert (
        len(v1_request_ids & v2_request_ids) == 0
    ), "V1 and V2 should have different request_ids"
    assert (
        len(v2_request_ids & v3_request_ids) == 0
    ), "V2 and V3 should have different request_ids"
    assert (
        len(v1_request_ids & v3_request_ids) == 0
    ), "V1 and V3 should have different request_ids"

    # Step 5: Verify each result has proper structure
    for results in [results_v1, results_v2, results_v3]:
        for result in results.agent_success_evaluation_results:
            assert result.result_id >= 0
            assert result.request_id != ""
            assert result.agent_version != ""
            assert isinstance(result.is_success, bool)
            assert isinstance(result.failure_type, str)
            assert isinstance(result.failure_reason, str)
            assert isinstance(result.agent_prompt_update, str)
            assert result.created_at > 0


@skip_in_precommit
def test_evaluate_regular_vs_shadow_content(
    reflexio_instance_agent_success_only: Reflexio,
    cleanup_agent_success_only: Callable[[], None],
):
    """Test agent success evaluation with regular vs shadow content comparison.

    This test verifies:
    1. Publishing interactions with shadow_content triggers comparison evaluation
    2. The regular_vs_shadow field is populated in evaluation results
    3. The comparison result is a valid RegularVsShadow enum value
    4. Both success evaluation and comparison work in a single LLM call
    """
    user_id = "test_user_shadow_comparison"
    agent_version = "test_agent_shadow"

    # Create interactions with shadow_content
    # Regular: Sales rep gives a brief response
    # Shadow: Sales rep gives a more detailed, helpful response
    interactions_with_shadow = [
        InteractionData(
            content="Hi, I'm looking for information about your software pricing.",
            role="Customer",
            user_action=UserActionType.NONE,
            user_action_description="",
            interacted_image_url="",
        ),
        InteractionData(
            content="Our pricing starts at $99/month.",
            shadow_content="Our pricing starts at $99/month for the basic plan, which includes 5 users and 10GB storage. We also have a Pro plan at $199/month with unlimited users and 100GB storage. Would you like me to explain the differences in more detail?",
            role="Sales Rep",
            user_action=UserActionType.NONE,
            user_action_description="",
            interacted_image_url="",
        ),
        InteractionData(
            content="That sounds interesting, can you tell me more?",
            role="Customer",
            user_action=UserActionType.NONE,
            user_action_description="",
            interacted_image_url="",
        ),
    ]

    # Step 1: Publish interactions with shadow content
    publish_response = reflexio_instance_agent_success_only.publish_interaction(
        {
            "user_id": user_id,
            "interaction_data_list": interactions_with_shadow,
            "source": "test_shadow_comparison",
            "agent_version": agent_version,
        }
    )
    assert publish_response.success is True

    # Step 2: Verify interactions were stored
    stored_interactions = (
        reflexio_instance_agent_success_only.request_context.storage.get_all_interactions()
    )
    assert len(stored_interactions) == len(interactions_with_shadow)

    # Verify shadow_content was stored
    shadow_interactions = [i for i in stored_interactions if i.shadow_content]
    assert len(shadow_interactions) > 0, "Shadow content should be stored"

    # Step 3: Get agent success evaluations
    get_response = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            GetAgentSuccessEvaluationResultsRequest(agent_version=agent_version)
        )
    )
    assert get_response.success is True
    assert len(get_response.agent_success_evaluation_results) > 0

    # Step 4: Verify regular_vs_shadow field is populated
    result = get_response.agent_success_evaluation_results[0]
    assert result.agent_version == agent_version
    assert isinstance(result.is_success, bool)

    # The regular_vs_shadow should be populated when shadow content exists
    assert (
        result.regular_vs_shadow is not None
    ), "regular_vs_shadow should be populated when shadow content exists"

    # Step 5: Verify the value is a valid RegularVsShadow enum
    valid_values = [e.value for e in RegularVsShadow]
    assert (
        result.regular_vs_shadow in valid_values
    ), f"regular_vs_shadow should be one of {valid_values}, got {result.regular_vs_shadow}"

    # Step 6: Log the comparison result for debugging
    print(f"Evaluation result: is_success={result.is_success}")
    print(f"Regular vs Shadow comparison: {result.regular_vs_shadow}")


@skip_in_precommit
@skip_low_priority
def test_evaluate_without_shadow_content(
    reflexio_instance_agent_success_only: Reflexio,
    sample_interaction_requests: list[InteractionData],
    cleanup_agent_success_only: Callable[[], None],
):
    """Test that regular_vs_shadow is None when no shadow content exists.

    This test verifies:
    1. Interactions without shadow_content don't trigger comparison
    2. The regular_vs_shadow field remains None
    3. Regular success evaluation still works correctly
    """
    user_id = "test_user_no_shadow"
    agent_version = "test_agent_no_shadow"

    # Step 1: Publish interactions WITHOUT shadow content
    publish_response = reflexio_instance_agent_success_only.publish_interaction(
        {
            "user_id": user_id,
            "interaction_data_list": sample_interaction_requests,
            "source": "test_no_shadow",
            "agent_version": agent_version,
        }
    )
    assert publish_response.success is True

    # Step 2: Get agent success evaluations
    get_response = (
        reflexio_instance_agent_success_only.get_agent_success_evaluation_results(
            GetAgentSuccessEvaluationResultsRequest(agent_version=agent_version)
        )
    )
    assert get_response.success is True
    assert len(get_response.agent_success_evaluation_results) > 0

    # Step 3: Verify regular_vs_shadow is None (no shadow content)
    result = get_response.agent_success_evaluation_results[0]
    assert result.agent_version == agent_version
    assert isinstance(result.is_success, bool)
    assert (
        result.regular_vs_shadow is None
    ), "regular_vs_shadow should be None when no shadow content exists"
